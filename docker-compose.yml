version: "3.9"

name: goresearch

networks:
  goresearch_net:
    driver: bridge
    internal: true

volumes:
  http_cache:
  llm_cache:
  models:
  reports:
  # Dedicated volume for secure-at-rest cache profile
  secure_cache:

services:
  # One-shot init to align ownership of named volumes with APP_UID:APP_GID
  perms-init:
    image: alpine:3.20@sha256:b3119ef930faabb6b7b976780c0c7a9c1aa24d0c75e9179ac10e6bc9ac080d0d
    user: "${APP_UID:-1000}:${APP_GID:-1000}"
    command: ["sh", "-lc", "chown -R ${APP_UID:-1000}:${APP_GID:-1000} /app/.goresearch-cache /app/reports || true"]
    volumes:
      - http_cache:/app/.goresearch-cache/http
      - llm_cache:/app/.goresearch-cache/llm
      - reports:/app/reports
    networks:
      - goresearch_net
    restart: "no"
    profiles: ["dev", "test", "offline"]

  # One-shot init for secure cache volume; aligns ownership for /app/.goresearch-cache
  perms-init-secure:
    image: alpine:3.20@sha256:b3119ef930faabb6b7b976780c0c7a9c1aa24d0c75e9179ac10e6bc9ac080d0d
    user: "${APP_UID:-1000}:${APP_GID:-1000}"
    command: ["sh", "-lc", "chown -R ${APP_UID:-1000}:${APP_GID:-1000} /app/.goresearch-cache /app/reports || true"]
    volumes:
      - secure_cache:/app/.goresearch-cache
      - reports:/app/reports
    networks:
      - goresearch_net
    restart: "no"
    profiles: ["secure-cache"]
  research-tool:
    # Runs the CLI from source using the official Go image.
    # A dedicated Dockerfile will be added in a later checklist item.
    image: golang:1.24-bookworm@sha256:2679c15c940573aded505b2f2fbbd4e718b5172327aae3ab9f43a10a5c700dfc
    working_dir: /app
    volumes:
      - ./:/app
      - http_cache:/app/.goresearch-cache/http
      - llm_cache:/app/.goresearch-cache/llm
      - reports:/app/reports
    # Load environment from .env (Compose default) and avoid in-file secrets.
    environment:
      - LLM_BASE_URL=http://llm-openai:8080/v1
      - LLM_MODEL=${LLM_MODEL:-gpt-neo}
      # Note: LLM_API_KEY and SEARX_KEY must be provided via environment/.env
      # and are intentionally not defaulted here to avoid baking secrets.
      - SEARX_URL=http://searxng:8080
    # Run as the invoking host user's UID:GID to avoid permission issues on bind mounts
    user: "${APP_UID:-1000}:${APP_GID:-1000}"
    command: ["sleep", "infinity"]
    networks:
      - goresearch_net
    profiles: ["dev"]
    depends_on:
      perms-init:
        condition: service_completed_successfully
      models-init:
        condition: service_completed_successfully
      llm-openai:
        condition: service_healthy
      searxng:
        condition: service_healthy

  # Test profile variant: no dependency on searxng; used for deterministic tests
  research-tool-test:
    image: golang:1.24-bookworm@sha256:2679c15c940573aded505b2f2fbbd4e718b5172327aae3ab9f43a10a5c700dfc
    working_dir: /app
    volumes:
      - ./:/app
      - http_cache:/app/.goresearch-cache/http
      - llm_cache:/app/.goresearch-cache/llm
      - reports:/app/reports
    environment:
      - LLM_BASE_URL=http://llm-openai:8080/v1
      - LLM_MODEL=${LLM_MODEL:-gpt-neo}
      # SEARX_URL intentionally omitted in test variant
    user: "${APP_UID:-1000}:${APP_GID:-1000}"
    command: ["sleep", "infinity"]
    networks:
      - goresearch_net
    profiles: ["test"]
    depends_on:
      perms-init:
        condition: service_completed_successfully
      models-init:
        condition: service_completed_successfully
      llm-openai:
        condition: service_healthy

  # Secure-at-rest variant: mounts a dedicated cache volume and enables strict perms
  research-tool-secure:
    image: golang:1.24-bookworm@sha256:2679c15c940573aded505b2f2fbbd4e718b5172327aae3ab9f43a10a5c700dfc
    working_dir: /app
    volumes:
      - ./:/app
      - secure_cache:/app/.goresearch-cache
      - reports:/app/reports
    environment:
      - LLM_BASE_URL=http://llm-openai:8080/v1
      - LLM_MODEL=${LLM_MODEL:-gpt-neo}
      - SEARX_URL=http://searxng:8080
      - CACHE_STRICT_PERMS=1
    user: "${APP_UID:-1000}:${APP_GID:-1000}"
    command: ["sleep", "infinity"]
    networks:
      - goresearch_net
    profiles: ["secure-cache"]
    depends_on:
      perms-init-secure:
        condition: service_completed_successfully
      models-init:
        condition: service_completed_successfully
      llm-openai:
        condition: service_healthy
      searxng:
        condition: service_healthy

  # Offline variant: runs in cache-only mode and does not depend on SearxNG.
  research-tool-offline:
    image: golang:1.24-bookworm@sha256:2679c15c940573aded505b2f2fbbd4e718b5172327aae3ab9f43a10a5c700dfc
    working_dir: /app
    volumes:
      - ./:/app
      - http_cache:/app/.goresearch-cache/http
      - llm_cache:/app/.goresearch-cache/llm
      - reports:/app/reports
    environment:
      - LLM_BASE_URL=http://llm-openai:8080/v1
      - LLM_MODEL=${LLM_MODEL:-gpt-neo}
      - HTTP_CACHE_ONLY=1
      - LLM_CACHE_ONLY=1
      # Intentionally omit SEARX_URL to disable SearxNG usage in app config
    # Run as the invoking host user's UID:GID to avoid permission issues on bind mounts
    user: "${APP_UID:-1000}:${APP_GID:-1000}"
    command: ["sleep", "infinity"]
    networks:
      - goresearch_net
    profiles: ["offline"]
    depends_on:
      perms-init:
        condition: service_completed_successfully
      models-init:
        condition: service_completed_successfully
      llm-openai:
        condition: service_healthy

  # OpenAI-compatible LLM server
  # This uses LocalAI as a convenient default for development.
  llm-openai:
    image: quay.io/go-skynet/local-ai@sha256:293f9109bdcebc2f75a10215cdff6683ee669fbdec9f6b85d101b19fe6702adb
    environment:
      - MODELS_PATH=/models
      - GIN_MODE=release
    networks:
      - goresearch_net
    volumes:
      - models:/models
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8080/v1/models"]
      interval: 10s
      timeout: 3s
      retries: 10
    profiles: ["dev", "test", "offline", "secure-cache"]
    depends_on:
      models-init:
        condition: service_completed_successfully

  # One-shot init to prepare models volume with checksum verification
  models-init:
    image: alpine:3.20@sha256:b3119ef930faabb6b7b976780c0c7a9c1aa24d0c75e9179ac10e6bc9ac080d0d
    command: ["sh", "-lc", "if [ -f /models/checksums.sha256 ]; then sha256sum -c /models/checksums.sha256; else echo 'no checksums provided'; fi"]
    volumes:
      - models:/models
    networks:
      - goresearch_net
    restart: "no"
    profiles: ["dev", "test", "secure-cache"]

  # SearxNG meta-search engine (default search provider)
  searxng:
    image: searxng/searxng@sha256:fe702750a9ffbf923533c67c456951a3be77f298915cf632077f3650ed4b5e4b
    networks:
      - goresearch_net
    environment:
      - SEARXNG_BASE_URL=http://searxng:8080/
      - SEARXNG_SETTINGS_PATH=/etc/searxng/settings.yml
    volumes:
      - ./devops/searxng-settings.yml:/etc/searxng/settings.yml:ro
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8080/status"]
      interval: 10s
      timeout: 3s
      retries: 10
    profiles: ["dev", "secure-cache"]

  # Stub LLM for deterministic tests (placeholder image; refined later)
  stub-llm:
    image: quay.io/go-skynet/local-ai@sha256:293f9109bdcebc2f75a10215cdff6683ee669fbdec9f6b85d101b19fe6702adb
    networks:
      - goresearch_net
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8080/v1/models"]
      interval: 10s
      timeout: 3s
      retries: 10
    profiles: ["test"]
